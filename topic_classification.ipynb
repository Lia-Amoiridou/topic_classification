{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classification with a Feedforward Network\n",
    "\n",
    "\n",
    "### \n",
    "\n",
    "\n",
    "The goal of this task is to develop a Feedforward neural network for topic classification. \n",
    "\n",
    "\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "- Text processing methods for transforming raw text data into input vectors for your network  \n",
    "\n",
    "\n",
    "- A Feedforward network consisting of:\n",
    "    - **One-hot** input layer mapping words into an **Embedding weight matrix** \n",
    "    - **One hidden layer** computing the mean embedding vector of all words in input followed by a **ReLU activation function** \n",
    "    - **Output layer** with a **softmax** activation. \n",
    "\n",
    "\n",
    "- The Stochastic Gradient Descent (SGD) algorithm with **back-propagation** to learn the weights of your Neural network. Your algorithm should:\n",
    "    - Use (and minimise) the **Categorical Cross-entropy loss** function \n",
    "    - Perform a **Forward pass** to compute intermediate outputs \n",
    "    - Perform a **Backward pass** to compute gradients and update all sets of weights \n",
    "    - Implement and use **Dropout** after each hidden layer for regularisation \n",
    "\n",
    "\n",
    "\n",
    "- Discuss how did you choose hyperparameters? You can tune the learning rate (hint: choose small values), embedding size {e.g. 50, 300, 500} and the dropout rate {e.g. 0.2, 0.5}. Please use tables or graphs to show training and validation performance for each hyperparameter combination  . \n",
    "\n",
    "\n",
    "\n",
    "- After training a model, plot the learning process (i.e. training and validation loss in each epoch) using a line plot and report accuracy. Does your model overfit, underfit or is about right? .\n",
    "\n",
    "\n",
    "\n",
    "- Re-train your network by using pre-trained embeddings ([GloVe](https://nlp.stanford.edu/projects/glove/)) trained on large corpora. Instead of randomly initialising the embedding weights matrix, you should initialise it with the pre-trained weights. During training, you should not update them (i.e. weight freezing) and backprop should stop before computing gradients for updating embedding weights. Report results by performing hyperparameter tuning and plotting the learning process. Do you get better performance? .\n",
    "\n",
    "\n",
    "\n",
    "- Extend you Feedforward network by adding more hidden layers (e.g. one more or two). How does it affect the performance? Note: You need to repeat hyperparameter tuning, but the number of combinations grows exponentially. Therefore, you need to choose a subset of all possible combinations \n",
    "\n",
    "\n",
    "- Provide well documented and commented code describing all of your choices. In general, you are free to make decisions about text processing (e.g. punctuation, numbers, vocabulary size) and hyperparameter values. We expect to see justifications and discussion for all of your choices. You must provide detailed explanations of your implementation, provide a detailed analysis of the results (e.g. why a model performs better than other models etc.) including error analyses (e.g. examples and discussion/analysis of missclasifications etc.)  . \n",
    "\n",
    "\n",
    "\n",
    "- Provide efficient solutions by using Numpy arrays when possible. Executing the whole notebook with your code should not take more than 10 minutes on any standard computer (e.g. Intel Core i5 CPU, 8 or 16GB RAM) excluding hyperparameter tuning runs and loading the pretrained vectors. You can find tips in Lab 1 . \n",
    "\n",
    "\n",
    "\n",
    "### Data \n",
    "\n",
    "The data you will use for the task is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "- `data_topic/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "- `data_topic/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_topic/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "Class 1: Politics, Class 2: Sports, Class 3: Economy\n",
    "\n",
    "### Pre-trained Embeddings\n",
    "\n",
    "You can download pre-trained GloVe embeddings trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) from [here](http://nlp.stanford.edu/data/glove.840B.300d.zip). No need to unzip, the file is large.\n",
    "\n",
    "### Save Memory\n",
    "\n",
    "To save RAM, when you finish each experiment you can delete the weights of your network using `del W` followed by Python's garbage collector `gc.collect()`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "You **must** submit a Jupyter Notebook file (assignment_yourusername.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex`, you need to have a Latex distribution installed e.g. MikTex or MacTex and pandoc). If you are unable to export the pdf via Latex, you can print the notebook web page to a pdf file from your browser (e.g. on Firefox: File->Print->Save to PDF).\n",
    "\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/3/library/index.html), NumPy, SciPy (excluding built-in softmax funtcions) and Pandas. You are **not allowed to use any third-party library** such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras, Pytorch etc.. You should mention if you've used Windows to write and test your code because we mostly use Unix based machines for marking (e.g. Ubuntu, MacOS). \n",
    "\n",
    "There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\\% or higher. The quality of the analysis of the results and discussion is as important as the implementation and accuracy of your models. Please be brief and consice in your discussion and analyses. \n",
    "\n",
    "This assignment will be marked out of 30. It is worth 30\\% of your final grade in the module.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Mon, 26 Apr 2023** and it needs to be submitted via Blackboard. Standard departmental penalties for lateness will be applied. We use a range of strategies to **detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index)**, including Turnitin which helps detect plagiarism. Use of unfair means would result in getting a failing grade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:00:18.625532Z",
     "start_time": "2020-04-02T15:00:17.377733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from time import localtime, strftime\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "import zipfile\n",
    "import gc\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Raw texts into training and development data\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.748484Z",
     "start_time": "2020-04-02T14:26:39.727404Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   label     2400 non-null   int64 \n",
      " 1   raw_text  2400 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data_topic/train.csv\",header=None,names=['label','raw_text'])\n",
    "test = pd.read_csv(\"data_topic/test.csv\",header=None,names=['label','raw_text'])\n",
    "dev = pd.read_csv(\"data_topic/dev.csv\",header=None,names=['label','raw_text'])\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input representations\n",
    "\n",
    "\n",
    "To train your Feedforward network, you first need to obtain input representations given a vocabulary. One-hot encoding requires large memory capacity. Therefore, we will instead represent documents as lists of vocabulary indices (each word corresponds to a vocabulary index). \n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of words. You should: \n",
    "- tokenise all texts into a list of unigrams (tip: you can re-use the functions from Assignment 1) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- remove unigrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of the top-N most frequent unigrams in the entire corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:40.851926Z",
     "start_time": "2020-04-02T14:26:40.847500Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i', 'if',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n",
    "              'but', 'not', 'there', 'no', 'does', 'not', 'so', 've', 'their',\n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T08:23:17.181553Z",
     "start_time": "2020-05-11T08:23:17.178314Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', \n",
    "                   stop_words=[], vocab=set()):\n",
    "    # Tokenize the raw text\n",
    "    pattern = re.compile(token_pattern)\n",
    "    x_token = pattern.findall(x_raw)\n",
    "    # Eliminating stop words\n",
    "    x_token = [token for token in x_token if token not in stop_words]\n",
    "    # Find all n-grams\n",
    "    x_ngrams = []\n",
    "    for n in range(1,ngram_range[1]+1):\n",
    "        x_ngram = zip(*[x_token[i:] for i in range(0,n)])\n",
    "        x_ngrams.extend([' '.join(tokens) for tokens in x_ngram])\n",
    "    # Extract features (vocabulary indices)\n",
    "    vocab = list(vocab)\n",
    "    x = [vocab.index(word_s) for word_s in x_ngrams if word_s in vocab]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:42.563876Z",
     "start_time": "2020-04-02T14:26:42.557967Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', min_df=0, keep_topN=0, stop_words=[]):\n",
    "    # Initialize vocabulary\n",
    "    vocab = set()\n",
    "    # Tokenization of the raw data\n",
    "    pattern = re.compile(token_pattern)\n",
    "    X_token = [pattern.findall(x_raw) for x_raw in X_raw]\n",
    "    # Removing of stop_words\n",
    "    X_token = [[token for token in x_token if token not in stop_words] for x_token in X_token]\n",
    "    # Create ngrams list\n",
    "    X_ngrams = []\n",
    "    # Loop through tokens\n",
    "    for x_token in X_token:\n",
    "        # Find ngrams and add them to the vocabulary\n",
    "        for n in range(1,ngram_range[1]+1):\n",
    "            x_ngram = zip(*[x_token[i:] for i in range(0,n)])\n",
    "            x_ngrams = [' '.join(tokens) for tokens in x_ngram]\n",
    "            X_ngrams.extend(x_ngrams)\n",
    "            for ngram in x_ngrams:\n",
    "                vocab.add(ngram)\n",
    "    # Create a dictionary of ngrams as keys and counts as values\n",
    "    counter = Counter(X_ngrams)\n",
    "    # Extract counts of each ngram\n",
    "    ngram_counts = [counter[ngram] for ngram in vocab]\n",
    "    # Sort it to find top N most frequent\n",
    "    sort_index = [i for i, x in sorted(enumerate(ngram_counts), key= lambda x: x[1], reverse=True)]\n",
    "    # Find no. of words in vocab to get the document frequencies\n",
    "    nwords = len(vocab)\n",
    "    # Sort ngram_counts to eliminate some of the top frequent ngrams\n",
    "    # Initialize the document frequency dictionary\n",
    "    df = {}\n",
    "    for index, ngram in enumerate(counter):\n",
    "        freq = counter[ngram]/nwords\n",
    "        df[ngram] = freq\n",
    "        if freq >= min_df:\n",
    "            if keep_topN > 0:\n",
    "                if not index in sort_index[:keep_topN]:\n",
    "                    vocab.remove(ngram)\n",
    "        else:\n",
    "            vocab.remove(ngram)\n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:43.577997Z",
     "start_time": "2020-04-02T14:26:43.478950Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab, df, ngram_counts = get_vocab(train['raw_text'].values, ngram_range=(1,1),\n",
    "                                token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', min_df=0, keep_topN=0, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and word -> vocabulary id dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:44.069661Z",
     "start_time": "2020-04-02T14:26:44.065058Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_word = {}\n",
    "word_id = {}\n",
    "id = 0\n",
    "for word in vocab:\n",
    "    id_word[id] = word\n",
    "    word_id[word] = id\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the list of unigrams  into a list of vocabulary indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing actual one-hot vectors into memory for all words in the entire data set is prohibitive. Instead, we will store word indices in the vocabulary and look-up the weight matrix. This is equivalent of doing a dot product between an one-hot vector and the weight matrix. \n",
    "\n",
    "First, represent documents in train, dev and test sets as lists of words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:45.047887Z",
     "start_time": "2020-04-02T14:26:44.920631Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = train['raw_text'].values\n",
    "x_test = test['raw_text'].values\n",
    "x_dev = dev['raw_text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert them into lists of indices in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:45.752658Z",
     "start_time": "2020-04-02T14:26:45.730409Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = [extract_ngrams(doc,ngram_range=(1,1),stop_words=stop_words,vocab=vocab) for doc in x_train]\n",
    "x_test = [extract_ngrams(doc,ngram_range=(1,1),stop_words=stop_words,vocab=vocab) for doc in x_test]\n",
    "x_dev = [extract_ngrams(doc,ngram_range=(1,1),stop_words=stop_words,vocab=vocab) for doc in x_dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the labels `Y` for train, dev and test sets into arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:03:13.183996Z",
     "start_time": "2020-04-02T15:03:13.077575Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = train['label'].values\n",
    "y_test = test['label'].values\n",
    "y_dev = dev['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "\n",
    "Your network should pass each word index into its corresponding embedding by looking-up on the embedding matrix and then compute the first hidden layer $\\mathbf{h}_1$:\n",
    "\n",
    "$$\\mathbf{h}_1 = \\frac{1}{|x|}\\sum_i W^e_i, i \\in x$$\n",
    "\n",
    "where $|x|$ is the number of words in the document and $W^e$ is an embedding matrix $|V|\\times d$, $|V|$ is the size of the vocabulary and $d$ the embedding size.\n",
    "\n",
    "Then $\\mathbf{h}_1$ should be passed through a ReLU activation function:\n",
    "\n",
    "$$\\mathbf{a}_1 = relu(\\mathbf{h}_1)$$\n",
    "\n",
    "Finally the hidden layer is passed to the output layer:\n",
    "\n",
    "\n",
    "$$\\mathbf{y} = \\text{softmax}(\\mathbf{a}_1W) $$ \n",
    "where $W$ is a matrix $d \\times |{\\cal Y}|$, $|{\\cal Y}|$ is the number of classes.\n",
    "\n",
    "During training, $\\mathbf{a}_1$ should be multiplied with a dropout mask vector (elementwise) for regularisation before it is passed to the output layer.\n",
    "\n",
    "You can extend to a deeper architecture by passing a hidden layer to another one:\n",
    "\n",
    "$$\\mathbf{h_i} = \\mathbf{a}_{i-1}W_i $$\n",
    "\n",
    "$$\\mathbf{a_i} = relu(\\mathbf{h_i}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Training\n",
    "\n",
    "First we need to define the parameters of our network by initiliasing the weight matrices. For that purpose, you should implement the `network_weights` function that takes as input:\n",
    "\n",
    "- `vocab_size`: the size of the vocabulary\n",
    "- `embedding_dim`: the size of the word embeddings\n",
    "- `hidden_dim`: a list of the sizes of any subsequent hidden layers. Empty if there are no hidden layers between the average embedding and the output layer \n",
    "- `num_classes`: the number of the classes for the output layer\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: a dictionary mapping from layer index (e.g. 0 for the embedding matrix) to the corresponding weight matrix initialised with small random numbers (hint: use numpy.random.uniform with from -0.1 to 0.1)\n",
    "\n",
    "Make sure that the dimensionality of each weight matrix is compatible with the previous and next weight matrix, otherwise you won't be able to perform forward and backward passes. Consider also using np.float32 precision to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:41:20.918617Z",
     "start_time": "2020-04-02T15:41:20.915597Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def network_weights(vocab_size=1000, embedding_dim=300, \n",
    "                    hidden_dim=[], num_classes=3, init_val = 0.1):\n",
    "    W = {}\n",
    "    # First, set the embedding matrix as the first layer\n",
    "    W[0] = np.random.uniform(-init_val,init_val,size=(vocab_size,embedding_dim))\n",
    "    # Then, add other hidden layers (if given)\n",
    "    size_prev = embedding_dim\n",
    "    if len(hidden_dim):\n",
    "        for i in range(len(hidden_dim)):\n",
    "            W[i+1] = np.random.uniform(-init_val,init_val,size=(size_prev,hidden_dim[i]))\n",
    "            size_prev = hidden_dim[i]\n",
    "    # Last, add the output layer\n",
    "    W[i+2 if len(hidden_dim) else 1] = np.random.uniform(-init_val,init_val,size=(size_prev,num_classes))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:48.636732Z",
     "start_time": "2020-04-02T14:26:48.634122Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = network_weights(vocab_size=3,embedding_dim=4,hidden_dim=[2], num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:31:57.970152Z",
     "start_time": "2020-04-01T10:31:57.966123Z"
    }
   },
   "source": [
    "Then you need to develop a `softmax` function (same as in Assignment 1) to be used in the output layer. \n",
    "\n",
    "It takes as input `z` (array of real numbers) and returns `sig` (the softmax of `z`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    sig = np.exp(z)/np.sum(np.exp(z))\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the categorical cross entropy loss by slightly modifying the function from Assignment 1 to depend only on the true label `y` and the class probabilities vector `y_preds`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:51.360838Z",
     "start_time": "2020-04-02T14:26:51.356935Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def categorical_loss(y, y_preds):\n",
    "    # map the true y label to one-hot\n",
    "    y_1hot = []\n",
    "    for i in range(len(y)):\n",
    "        arr = list(np.zeros(len(y_preds[0])))\n",
    "        arr[y[i]-1] = 1\n",
    "        y_1hot.append(arr)\n",
    "    l = np.sum(-np.multiply(y_1hot, np.log(y_preds)), axis=1)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:02:56.149535Z",
     "start_time": "2020-03-31T15:02:56.145738Z"
    }
   },
   "source": [
    "Then, implement the `relu` function to introduce non-linearity after each hidden layer of your network \n",
    "(during the forward pass): \n",
    "\n",
    "$$relu(z_i)= max(z_i,0)$$\n",
    "\n",
    "and the `relu_derivative` function to compute its derivative (used in the backward pass):\n",
    "\n",
    "  \n",
    "  relu_derivative($z_i$)=0, if $z_i$<=0, 1 otherwise.\n",
    "  \n",
    "\n",
    "\n",
    "Note that both functions take as input a vector $z$ \n",
    "\n",
    "Hint use .copy() to avoid in place changes in array z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:52.665236Z",
     "start_time": "2020-04-02T14:26:52.661519Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    a = np.maximum(z,0)\n",
    "    return a\n",
    "    \n",
    "def relu_derivative(z):\n",
    "    a = relu(z)\n",
    "    dz = a.copy()\n",
    "    dz[a>0] = 1\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training you should also apply a dropout mask element-wise after the activation function (i.e. vector of ones with a random percentage set to zero). The `dropout_mask` function takes as input:\n",
    "\n",
    "- `size`: the size of the vector that we want to apply dropout\n",
    "- `dropout_rate`: the percentage of elements that will be randomly set to zeros\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `dropout_vec`: a vector with binary values (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.429192Z",
     "start_time": "2020-04-02T14:26:53.425301Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dropout_mask(size, dropout_rate):\n",
    "    dropout_vec = (np.random.rand(size) > dropout_rate)/1\n",
    "    return dropout_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.853632Z",
     "start_time": "2020-04-02T14:26:53.849944Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(dropout_mask(10, 0.2))\n",
    "print(dropout_mask(10, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the `forward_pass` function that passes the input x through the network up to the output layer for computing the probability for each class using the weight matrices in `W`. The ReLU activation function should be applied on each hidden layer. \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `dropout_rate`: the dropout rate that is used to generate a random dropout mask vector applied after each hidden layer for regularisation.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `out_vals`: a dictionary of output values from each layer: h (the vector before the activation function), a (the resulting vector after passing h from the activation function), its dropout mask vector; and the prediction vector (probability for each class) from the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:54.761268Z",
     "start_time": "2020-04-02T14:26:54.753402Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, W, dropout_rate=0.2):\n",
    "    # Create empty lists for output values\n",
    "    h_vecs = []\n",
    "    a_vecs = []\n",
    "    dropout_vecs = []\n",
    "    # First Layer: Embedding => h1 (1xd)\n",
    "    h1 = np.expand_dims(np.mean(W[0][x],axis=0),axis=0)\n",
    "    h_vecs.append(h1)\n",
    "    # Loop through all hidden layers\n",
    "    h_i = h1\n",
    "    for i in range(1,len(W)):\n",
    "        # Activation of the previous layer (relu) => a1 (1xd_i)\n",
    "        a_i = relu(h_i)\n",
    "        # Dropout of the last output => a1_d (1xd_i)\n",
    "        dmask_i = dropout_mask(a_i.shape[1],dropout_rate)\n",
    "        ai_d = np.multiply(a_i,dmask_i)\n",
    "        a_vecs.append(ai_d)\n",
    "        dropout_vecs.append(dmask_i)\n",
    "        # Output of the hidden layer => h_i (1xd_i+1)\n",
    "        h_i = np.matmul(ai_d,W[i])\n",
    "        h_vecs.append(h_i)\n",
    "    # Softmax activation for the output => y_hat (1xc)\n",
    "    pred = np.squeeze(softmax(h_i)).tolist()\n",
    "    out_vals = {'h':h_vecs,'a':a_vecs,'d':dropout_vecs,'y_pred':pred}\n",
    "    return out_vals    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward_pass` function computes the gradients and updates the weights for each matrix in the network from the output to the input. It takes as input \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `y`: the true label\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `out_vals`: a dictionary of output values from a forward pass.\n",
    "- `learning_rate`: the learning rate for updating the weights.\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: the updated weights of the network.\n",
    "\n",
    "Hint: the gradients on the output layer are similar to the multiclass logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T08:24:13.732705Z",
     "start_time": "2020-05-11T08:24:13.729741Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, y, W, out_vals, lr=0.001, freeze_emb=False):\n",
    "    # Initialize gradients\n",
    "    dL_dW = {}\n",
    "    # Apply one-hot mapping to the y label\n",
    "    arr = list(np.zeros(len(out_vals['y_pred'])))\n",
    "    arr[y-1] = 1\n",
    "    y = np.expand_dims(arr,axis=0)\n",
    "    # First: Gradient of loss w.r.t h_last (1xc)\n",
    "    dL_dhlast = out_vals['y_pred'] - y\n",
    "    # Loop through all hidden layers\n",
    "    dL_dhi = dL_dhlast\n",
    "    for i in range(len(W)-1,0,-1):\n",
    "        # Gradient of loss w.r.t W2 (d_ixd_i+1)\n",
    "        dL_dW[i] = np.matmul(out_vals['a'][i-1].T, dL_dhi)\n",
    "        # Gradient of loss w.r.t a1 (1xd)\n",
    "        dL_dai = np.multiply(np.matmul(dL_dhi,W[i].T), out_vals['d'][i-1])\n",
    "        # Gradient of loss w.r.t h1 (1xd)\n",
    "        dL_dhi = np.multiply(dL_dai, relu_derivative(out_vals['h'][i-1]))\n",
    "    # Last: Gradient of loss w.r.t W1\n",
    "    dL_dW[0] = 1/len(x)*dL_dhi\n",
    "    # Update Gradients\n",
    "    if not freeze_emb:\n",
    "        W[0][x] = W[0][x] - lr*dL_dW[0]\n",
    "    for i in range(1,len(W)):\n",
    "        W[i] = W[i] - lr*dL_dW[i]\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support back-propagation by using the `forward_pass` and `backward_pass` functions.\n",
    "\n",
    "The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `W`: the weights of the network (dictionary)\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `dropout`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated (to be used by the backward pass function).\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:19.021428Z",
     "start_time": "2020-04-02T15:09:19.017835Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, W, X_dev=[], Y_dev=[], lr=0.001, \n",
    "        dropout=0.2, epochs=5, tolerance=0.001, freeze_emb=False, print_progress=True):\n",
    "    # Initialize histories\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    # Find data sizes\n",
    "    m_train = len(Y_tr)\n",
    "    m_dev = len(Y_dev)\n",
    "    # Create train Dataset\n",
    "    D_train = list(zip(X_tr, Y_tr))\n",
    "    # Initialize loss history to infinity\n",
    "    val_loss_old = np.inf\n",
    "    # Loop through epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Randomize order in train dataset\n",
    "        random.shuffle(D_train)\n",
    "        Y_tr = [d_train[1] for d_train in D_train]\n",
    "        Y_hat = []\n",
    "        # loop through data_set\n",
    "        for (x_tr, y_tr) in D_train:\n",
    "            # First apply forward pass\n",
    "            out_vals = forward_pass(x_tr, W, dropout)\n",
    "            Y_hat.append(out_vals['y_pred'])\n",
    "            # Then apply backward pass\n",
    "            W = backward_pass(x_tr, y_tr, W, out_vals, lr, freeze_emb)\n",
    "        # Monitor training and validation losses\n",
    "        training_loss = 1/m_train*sum(categorical_loss(Y_tr, Y_hat))\n",
    "        training_loss_history.append(training_loss)\n",
    "        Yh_dev = []\n",
    "        for x_dev in X_dev:\n",
    "            out_vals = forward_pass(x_dev, W, dropout)\n",
    "            Yh_dev.append(out_vals['y_pred'])\n",
    "        validation_loss = 1/m_dev*sum(categorical_loss(Y_dev, Yh_dev))\n",
    "        validation_loss_history.append(validation_loss)\n",
    "        if print_progress:\n",
    "            print(\"For epoch no. %d:\"%(epoch+1))\n",
    "            print(\" Training loss = %s\"%training_loss)\n",
    "            print(\" Validation loss = %s\"%validation_loss)\n",
    "        if abs(validation_loss - val_loss_old) < tolerance:\n",
    "           break\n",
    "        val_loss_old = validation_loss\n",
    "    return W, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate your neural net. First, you need to define your network using the `network_weights` function followed by SGD with backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:33.643515Z",
     "start_time": "2020-04-02T15:09:33.640943Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (9855, 200)\n",
      "Shape W1 (200, 3)\n",
      "For epoch no. 1:\n",
      " Training loss = 1.0984783983046016\n",
      " Validation loss = 1.0990419051521694\n",
      "For epoch no. 2:\n",
      " Training loss = 1.0965882632760313\n",
      " Validation loss = 1.0968171489676073\n",
      "For epoch no. 3:\n",
      " Training loss = 1.0942154736378873\n",
      " Validation loss = 1.0953856369626476\n",
      "For epoch no. 4:\n",
      " Training loss = 1.0912806148515268\n",
      " Validation loss = 1.0934303898430113\n",
      "For epoch no. 5:\n",
      " Training loss = 1.0867377808192946\n",
      " Validation loss = 1.0893897035703004\n",
      "For epoch no. 6:\n",
      " Training loss = 1.079808309043491\n",
      " Validation loss = 1.0834513772087357\n",
      "For epoch no. 7:\n",
      " Training loss = 1.0686742455368663\n",
      " Validation loss = 1.0750779325358775\n",
      "For epoch no. 8:\n",
      " Training loss = 1.0503428286277667\n",
      " Validation loss = 1.0599841421635132\n",
      "For epoch no. 9:\n",
      " Training loss = 1.0208908003699217\n",
      " Validation loss = 1.0351760269568695\n",
      "For epoch no. 10:\n",
      " Training loss = 0.9769677774296655\n",
      " Validation loss = 0.9999774744460393\n",
      "For epoch no. 11:\n",
      " Training loss = 0.9183722852366059\n",
      " Validation loss = 0.9559720794228094\n",
      "For epoch no. 12:\n",
      " Training loss = 0.8492397159172375\n",
      " Validation loss = 0.900890188543796\n",
      "For epoch no. 13:\n",
      " Training loss = 0.7780473611899286\n",
      " Validation loss = 0.8401359447470393\n",
      "For epoch no. 14:\n",
      " Training loss = 0.7071554556320747\n",
      " Validation loss = 0.774383807719216\n",
      "For epoch no. 15:\n",
      " Training loss = 0.642032925559377\n",
      " Validation loss = 0.7168827838165666\n",
      "For epoch no. 16:\n",
      " Training loss = 0.5811227742578671\n",
      " Validation loss = 0.6610989465469339\n",
      "For epoch no. 17:\n",
      " Training loss = 0.5238978025580429\n",
      " Validation loss = 0.6025374364307243\n",
      "For epoch no. 18:\n",
      " Training loss = 0.47137435690955604\n",
      " Validation loss = 0.5549085658781474\n",
      "For epoch no. 19:\n",
      " Training loss = 0.4261317711214271\n",
      " Validation loss = 0.5149477760992464\n",
      "For epoch no. 20:\n",
      " Training loss = 0.38890714384182645\n",
      " Validation loss = 0.4905596591280117\n",
      "For epoch no. 21:\n",
      " Training loss = 0.35769455157751906\n",
      " Validation loss = 0.4688228465443582\n",
      "For epoch no. 22:\n",
      " Training loss = 0.32873003769036613\n",
      " Validation loss = 0.4515479916876943\n",
      "For epoch no. 23:\n",
      " Training loss = 0.30509955900527735\n",
      " Validation loss = 0.4299462002699329\n",
      "For epoch no. 24:\n",
      " Training loss = 0.28149567845568557\n",
      " Validation loss = 0.41191140576240465\n",
      "For epoch no. 25:\n",
      " Training loss = 0.26127464189675653\n",
      " Validation loss = 0.3974154586988761\n",
      "For epoch no. 26:\n",
      " Training loss = 0.24093327121470776\n",
      " Validation loss = 0.37980656923171763\n",
      "For epoch no. 27:\n",
      " Training loss = 0.2241812929552702\n",
      " Validation loss = 0.37043936540266714\n",
      "For epoch no. 28:\n",
      " Training loss = 0.20802614472932882\n",
      " Validation loss = 0.3654757915990127\n",
      "For epoch no. 29:\n",
      " Training loss = 0.19482604324186886\n",
      " Validation loss = 0.3602349989365958\n",
      "For epoch no. 30:\n",
      " Training loss = 0.18042780973333758\n",
      " Validation loss = 0.34449201834914395\n",
      "For epoch no. 31:\n",
      " Training loss = 0.16793990980389095\n",
      " Validation loss = 0.33469577813204293\n",
      "For epoch no. 32:\n",
      " Training loss = 0.16113076299779486\n",
      " Validation loss = 0.3325646853329576\n",
      "For epoch no. 33:\n",
      " Training loss = 0.14990659074278656\n",
      " Validation loss = 0.32283645356907054\n",
      "For epoch no. 34:\n",
      " Training loss = 0.1403607636585463\n",
      " Validation loss = 0.3185410324005364\n",
      "For epoch no. 35:\n",
      " Training loss = 0.13311058278526638\n",
      " Validation loss = 0.32701921425714076\n",
      "For epoch no. 36:\n",
      " Training loss = 0.12647466663784437\n",
      " Validation loss = 0.3062987645253856\n",
      "For epoch no. 37:\n",
      " Training loss = 0.11875293590739887\n",
      " Validation loss = 0.31661450592928536\n",
      "For epoch no. 38:\n",
      " Training loss = 0.11532026655077544\n",
      " Validation loss = 0.2969326523187109\n",
      "For epoch no. 39:\n",
      " Training loss = 0.10602318381186975\n",
      " Validation loss = 0.3017047983041584\n",
      "For epoch no. 40:\n",
      " Training loss = 0.1016515473657715\n",
      " Validation loss = 0.3074230010200732\n",
      "For epoch no. 41:\n",
      " Training loss = 0.09698071322267776\n",
      " Validation loss = 0.3014433413866136\n",
      "For epoch no. 42:\n",
      " Training loss = 0.09134032212893371\n",
      " Validation loss = 0.3081599835504329\n",
      "For epoch no. 43:\n",
      " Training loss = 0.08747330616473706\n",
      " Validation loss = 0.27454623750257656\n",
      "For epoch no. 44:\n",
      " Training loss = 0.0856679617187007\n",
      " Validation loss = 0.2928171216393887\n",
      "For epoch no. 45:\n",
      " Training loss = 0.08050418883496505\n",
      " Validation loss = 0.28143228805295145\n",
      "For epoch no. 46:\n",
      " Training loss = 0.07880553839890288\n",
      " Validation loss = 0.28622763495910825\n",
      "For epoch no. 47:\n",
      " Training loss = 0.07440359865992387\n",
      " Validation loss = 0.2958395719466182\n",
      "For epoch no. 48:\n",
      " Training loss = 0.072372281948046\n",
      " Validation loss = 0.2781492057906863\n",
      "For epoch no. 49:\n",
      " Training loss = 0.06850351735125705\n",
      " Validation loss = 0.2890915279087271\n",
      "For epoch no. 50:\n",
      " Training loss = 0.06579492210136535\n",
      " Validation loss = 0.2888355591595423\n",
      "[0.9675835624930689, 0.018451976113283617, 0.013964461393647503]\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(vocab),embedding_dim=200,\n",
    "                    hidden_dim=[], num_classes=3)\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "W, loss_tr, dev_loss = SGD(x_train, y_train,\n",
    "                            W,\n",
    "                            X_dev=x_dev, \n",
    "                            Y_dev=y_dev,\n",
    "                            lr=0.02, \n",
    "                            dropout=0.2,\n",
    "                            freeze_emb=False,\n",
    "                            tolerance=0.001,\n",
    "                            epochs=100)\n",
    "out = forward_pass(x_test[0],W)\n",
    "print(out['y_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:15.716497Z",
     "start_time": "2020-04-02T14:27:15.612736Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iters = range(0,len(loss_tr))\n",
    "plt.plot(iters,loss_tr)\n",
    "plt.plot(iters,dev_loss)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['train loss','validation loss'])\n",
    "plt.title(\"Learning Progress\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:10:11.037495Z",
     "start_time": "2020-04-02T15:10:11.034999Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8611111111111112\n",
      "Precision: 0.8639426224629486\n",
      "Recall: 0.8611111111111112\n",
      "F1-Score: 0.8609628417730976\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y_pred'])+1 for x in x_test]\n",
    "print('Accuracy:', accuracy_score(y_test,preds_te))\n",
    "print('Precision:', precision_score(y_test,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(y_test,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(y_test,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, hyperparameters are tuned based on the validation results obtained during multiple runs of a machine learning model. The performance enhancement observed during such runs can be attributed to modifications made in the hyperparameters. The results obtained from various runs are summarized in the following table:\n",
    "|# Run|# Number of Epochs|Hidden layer sizes|Learning rate|Dropout|Tolerance|Training loss|Validation loss| Comments\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|1|2|10|0.01|0.1|0.001|1.099|1.098|Training stopped too early, decrease tolerance|\n",
    "|2|40|10|0.01|0.1|0.0001|0.642|0.735|High losses, increase embedding size|\n",
    "|3|100|50|0.01|0.1|0.0001|0.096|0.301|Slow learning progress, increase learning rate|\n",
    "|4|100|50|0.1|0.1|0.0001|0.009|0.420|Overfitting, increase dropout|\n",
    "|5|100|50|0.1|0.1|0.0001|0.005|0.461|Overfitting, decrease learning rate|\n",
    "|6|100|50|0.02|0.1|0.0001|0.04|0.293|Increase embedding size to learn new features|\n",
    "|7|100|200|0.02|0.1|0.0001|0.019|0.289|Overfitting, increase dropout|\n",
    "|8|100|200|0.02|0.2|0.0001|0.027|0.276|Add early stopping effect, increase tolerance|\n",
    "|9|50|200|0.02|0.2|0.001|0.065|0.288|-|\n",
    "\n",
    "The obtained results suggest that the model shows good performance, however, there is scope for further improvement. In particular, the model appears to suffer from overfitting, which can be addressed by increasing the training data size or using a deeper architecture that includes more features to better understand the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3215"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del W\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained Embeddings\n",
    "\n",
    "Now re-train the network using GloVe pre-trained embeddings. You need to modify the `backward_pass` function above to stop computing gradients and updating weights of the embedding matrix.\n",
    "\n",
    "Use the function below to obtain the embedding martix for your vocabulary. Generally, that should work without any problem. If you get errors, you can modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:32.020697Z",
     "start_time": "2020-04-02T14:27:32.015733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_glove_embeddings(f_zip, f_txt, word2id, emb_size=300):\n",
    "    \n",
    "    w_emb = np.zeros((len(word2id), emb_size))\n",
    "    \n",
    "    with zipfile.ZipFile(f_zip) as z:\n",
    "        with z.open(f_txt) as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                word = line.split()[0]\n",
    "                     \n",
    "                if word in vocab:\n",
    "                    emb = line.strip('\\n').split()[1:]\n",
    "                    emb = ['0' if x == 'Killerseats.com' else x for x in emb]\n",
    "                    emb = np.array(emb).astype(np.float32)[:emb_size]\n",
    "                    w_emb[word2id[word]] +=emb\n",
    "    return w_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:28:54.548613Z",
     "start_time": "2020-04-02T14:27:32.780248Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_glove = get_glove_embeddings(\"glove.840B.300d.zip\",\"glove.840B.300d.txt\",word_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialise the weights of your network using the `network_weights` function. Second, replace the weigths of the embedding matrix with `w_glove`. Finally, train the network by freezing the embedding weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:30:11.121198Z",
     "start_time": "2020-04-02T14:29:24.946124Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (9855, 300)\n",
      "Shape W1 (300, 3)\n",
      "For epoch no. 1:\n",
      " Training loss = 0.5537763546340528\n",
      " Validation loss = 0.3732965790904515\n",
      "For epoch no. 2:\n",
      " Training loss = 0.37652106230220433\n",
      " Validation loss = 0.2656222866130047\n",
      "For epoch no. 3:\n",
      " Training loss = 0.3412321632239131\n",
      " Validation loss = 0.2428270687379156\n",
      "For epoch no. 4:\n",
      " Training loss = 0.3280640412551002\n",
      " Validation loss = 0.22340464333146468\n",
      "For epoch no. 5:\n",
      " Training loss = 0.3134545874748036\n",
      " Validation loss = 0.2619400713756511\n",
      "For epoch no. 6:\n",
      " Training loss = 0.3054255127854148\n",
      " Validation loss = 0.22146053361759477\n",
      "For epoch no. 7:\n",
      " Training loss = 0.30272401377596814\n",
      " Validation loss = 0.20357018847734837\n",
      "For epoch no. 8:\n",
      " Training loss = 0.2880278550729047\n",
      " Validation loss = 0.20941082958064933\n",
      "For epoch no. 9:\n",
      " Training loss = 0.28794549789826857\n",
      " Validation loss = 0.20506185376087943\n",
      "For epoch no. 10:\n",
      " Training loss = 0.2795687838943836\n",
      " Validation loss = 0.19751695969529617\n",
      "For epoch no. 11:\n",
      " Training loss = 0.2772849474243829\n",
      " Validation loss = 0.19438499746980656\n",
      "For epoch no. 12:\n",
      " Training loss = 0.2782939180207284\n",
      " Validation loss = 0.18933876172577252\n",
      "For epoch no. 13:\n",
      " Training loss = 0.2754274250696303\n",
      " Validation loss = 0.1913229809867403\n",
      "For epoch no. 14:\n",
      " Training loss = 0.2710681898525293\n",
      " Validation loss = 0.18395633962024663\n",
      "For epoch no. 15:\n",
      " Training loss = 0.26646576683863016\n",
      " Validation loss = 0.19305109009395804\n",
      "For epoch no. 16:\n",
      " Training loss = 0.26258751903033606\n",
      " Validation loss = 0.21729822765566698\n",
      "For epoch no. 17:\n",
      " Training loss = 0.26111253601990647\n",
      " Validation loss = 0.18926167710321573\n",
      "For epoch no. 18:\n",
      " Training loss = 0.26025427457262507\n",
      " Validation loss = 0.18136035583417834\n",
      "For epoch no. 19:\n",
      " Training loss = 0.2592536789861554\n",
      " Validation loss = 0.1776352188215633\n",
      "For epoch no. 20:\n",
      " Training loss = 0.2548880746320869\n",
      " Validation loss = 0.17461974417678325\n",
      "For epoch no. 21:\n",
      " Training loss = 0.25494175890739607\n",
      " Validation loss = 0.17079308185669492\n",
      "For epoch no. 22:\n",
      " Training loss = 0.2556496880828957\n",
      " Validation loss = 0.19072620803643356\n",
      "For epoch no. 23:\n",
      " Training loss = 0.24899393004781428\n",
      " Validation loss = 0.17285987055639668\n",
      "For epoch no. 24:\n",
      " Training loss = 0.2500023294238585\n",
      " Validation loss = 0.20268435091997739\n",
      "For epoch no. 25:\n",
      " Training loss = 0.24961795614933574\n",
      " Validation loss = 0.16739988209771545\n",
      "For epoch no. 26:\n",
      " Training loss = 0.24370937579486188\n",
      " Validation loss = 0.1858033092572415\n",
      "For epoch no. 27:\n",
      " Training loss = 0.2425694123051139\n",
      " Validation loss = 0.19000391345989653\n",
      "For epoch no. 28:\n",
      " Training loss = 0.2419363346999018\n",
      " Validation loss = 0.16152058343412218\n",
      "For epoch no. 29:\n",
      " Training loss = 0.24008968341073025\n",
      " Validation loss = 0.23196143980746248\n",
      "For epoch no. 30:\n",
      " Training loss = 0.23859776046951062\n",
      " Validation loss = 0.1717202185305684\n",
      "For epoch no. 31:\n",
      " Training loss = 0.23816976382329247\n",
      " Validation loss = 0.1739354412465077\n",
      "For epoch no. 32:\n",
      " Training loss = 0.23466078670569143\n",
      " Validation loss = 0.16077560883265934\n",
      "For epoch no. 33:\n",
      " Training loss = 0.2367174404323489\n",
      " Validation loss = 0.20353443040102975\n",
      "For epoch no. 34:\n",
      " Training loss = 0.2322214567463758\n",
      " Validation loss = 0.1955135496355639\n",
      "For epoch no. 35:\n",
      " Training loss = 0.23341530188742662\n",
      " Validation loss = 0.1792470408207718\n",
      "For epoch no. 36:\n",
      " Training loss = 0.23129368405322948\n",
      " Validation loss = 0.15973317019672398\n",
      "For epoch no. 37:\n",
      " Training loss = 0.2290970851502408\n",
      " Validation loss = 0.1529260711265832\n",
      "For epoch no. 38:\n",
      " Training loss = 0.22885099299672956\n",
      " Validation loss = 0.18361349312439984\n",
      "For epoch no. 39:\n",
      " Training loss = 0.2332715159866463\n",
      " Validation loss = 0.21156082120883615\n",
      "For epoch no. 40:\n",
      " Training loss = 0.23049310677239315\n",
      " Validation loss = 0.16452857456696537\n",
      "For epoch no. 41:\n",
      " Training loss = 0.22337758184742645\n",
      " Validation loss = 0.1874773643564854\n",
      "For epoch no. 42:\n",
      " Training loss = 0.22528050470841232\n",
      " Validation loss = 0.19883783772499744\n",
      "For epoch no. 43:\n",
      " Training loss = 0.23076392403748583\n",
      " Validation loss = 0.1949785642465733\n",
      "For epoch no. 44:\n",
      " Training loss = 0.22506880859391487\n",
      " Validation loss = 0.18100320180567028\n",
      "For epoch no. 45:\n",
      " Training loss = 0.2289620342048205\n",
      " Validation loss = 0.17346499400704846\n",
      "For epoch no. 46:\n",
      " Training loss = 0.2224016008327805\n",
      " Validation loss = 0.17850985690509835\n",
      "For epoch no. 47:\n",
      " Training loss = 0.22052625871186726\n",
      " Validation loss = 0.18274717633320556\n",
      "For epoch no. 48:\n",
      " Training loss = 0.224418249407436\n",
      " Validation loss = 0.1801174183302872\n",
      "For epoch no. 49:\n",
      " Training loss = 0.22177238131246507\n",
      " Validation loss = 0.19725042258003372\n",
      "For epoch no. 50:\n",
      " Training loss = 0.22522332017737337\n",
      " Validation loss = 0.17462101707682767\n",
      "For epoch no. 51:\n",
      " Training loss = 0.22111136323955682\n",
      " Validation loss = 0.18898456995291815\n",
      "For epoch no. 52:\n",
      " Training loss = 0.22270133426489547\n",
      " Validation loss = 0.18073947594556913\n",
      "For epoch no. 53:\n",
      " Training loss = 0.22368342684510462\n",
      " Validation loss = 0.2151590657116928\n",
      "For epoch no. 54:\n",
      " Training loss = 0.22146267332988256\n",
      " Validation loss = 0.17235483123061904\n",
      "For epoch no. 55:\n",
      " Training loss = 0.21213204840017544\n",
      " Validation loss = 0.18593739097207412\n",
      "For epoch no. 56:\n",
      " Training loss = 0.22145509059927795\n",
      " Validation loss = 0.18007897370956222\n",
      "For epoch no. 57:\n",
      " Training loss = 0.21833068753516086\n",
      " Validation loss = 0.1980580535053769\n",
      "For epoch no. 58:\n",
      " Training loss = 0.21917678938003024\n",
      " Validation loss = 0.22112983136485417\n",
      "For epoch no. 59:\n",
      " Training loss = 0.21942106127037278\n",
      " Validation loss = 0.16629338359308268\n",
      "For epoch no. 60:\n",
      " Training loss = 0.21757768533058328\n",
      " Validation loss = 0.1517267092584938\n",
      "For epoch no. 61:\n",
      " Training loss = 0.21277221287899928\n",
      " Validation loss = 0.1713240333404392\n",
      "For epoch no. 62:\n",
      " Training loss = 0.2211645683377825\n",
      " Validation loss = 0.2023406130455097\n",
      "For epoch no. 63:\n",
      " Training loss = 0.21726752111661687\n",
      " Validation loss = 0.16612566170332066\n",
      "For epoch no. 64:\n",
      " Training loss = 0.21672554053283635\n",
      " Validation loss = 0.17963838020218387\n",
      "For epoch no. 65:\n",
      " Training loss = 0.21603206159822994\n",
      " Validation loss = 0.1785643345190146\n",
      "For epoch no. 66:\n",
      " Training loss = 0.21751315469782545\n",
      " Validation loss = 0.20082706673375242\n",
      "For epoch no. 67:\n",
      " Training loss = 0.21795638391495994\n",
      " Validation loss = 0.20203146492477458\n",
      "For epoch no. 68:\n",
      " Training loss = 0.20843808796846441\n",
      " Validation loss = 0.15911420849926253\n",
      "For epoch no. 69:\n",
      " Training loss = 0.2116280158935428\n",
      " Validation loss = 0.20300103269970754\n",
      "For epoch no. 70:\n",
      " Training loss = 0.21173367195299742\n",
      " Validation loss = 0.18179174345130708\n",
      "For epoch no. 71:\n",
      " Training loss = 0.2089442871769579\n",
      " Validation loss = 0.1874031037692667\n",
      "For epoch no. 72:\n",
      " Training loss = 0.21699740535116524\n",
      " Validation loss = 0.17146165850699852\n",
      "For epoch no. 73:\n",
      " Training loss = 0.21074192450030999\n",
      " Validation loss = 0.17865140668741825\n",
      "For epoch no. 74:\n",
      " Training loss = 0.20883284236451097\n",
      " Validation loss = 0.1917344432082833\n",
      "For epoch no. 75:\n",
      " Training loss = 0.20798949738200426\n",
      " Validation loss = 0.15515232277262256\n",
      "For epoch no. 76:\n",
      " Training loss = 0.20686313421584535\n",
      " Validation loss = 0.17709072255208844\n",
      "For epoch no. 77:\n",
      " Training loss = 0.21099831035964198\n",
      " Validation loss = 0.18271121657318865\n",
      "For epoch no. 78:\n",
      " Training loss = 0.20867106715673256\n",
      " Validation loss = 0.18327234019637942\n",
      "[0.8554624942412727, 0.14003831723099303, 0.0044991885277342394]\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(vocab),embedding_dim=300,\n",
    "                    hidden_dim=[], num_classes=3)\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "W[0] = w_glove\n",
    "\n",
    "W, loss_tr, dev_loss = SGD(x_train, y_train,\n",
    "                            W,\n",
    "                            X_dev=x_dev, \n",
    "                            Y_dev=y_dev,\n",
    "                            lr=0.04, \n",
    "                            dropout=0.1,\n",
    "                            freeze_emb=True,\n",
    "                            tolerance=0.001,\n",
    "                            epochs=100)\n",
    "out = forward_pass(x_test[0],W)\n",
    "print(out['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:12:00.815184Z",
     "start_time": "2020-04-02T15:12:00.812563Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8811111111111111\n",
      "Precision: 0.8822274398915694\n",
      "Recall: 0.8811111111111112\n",
      "F1-Score: 0.8807239973275397\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y_pred'])+1 for x in x_test]\n",
    "print('Accuracy:', accuracy_score(y_test,preds_te))\n",
    "print('Precision:', precision_score(y_test,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(y_test,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(y_test,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, adjusting hyperparameters is a common approach to optimize the performance of machine learning models. By monitoring validation results during the training process, we can obtain valuable insights into how changes in hyperparameters affect the model's performance. Here, we present the results of a series of runs where different hyperparameters were explored.\n",
    "The experiments were conducted with a model that uses embeddings. The table below summarizes the hyperparameters used in each run, as well as the resulting training and validation losses. The comments column describes any relevant observations or insights gained during the experiment.\n",
    "|# Run|# Number of Epochs|Hidden layer sizes|Learning rate|Dropout|Tolerance|Training loss|Validation loss| Comments\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|1|25|200|0.02|0.2|0.001|0.323|0.238|Relatively large losses, Increase embedding size to improve performance|\n",
    "|2|31|200|0.02|0.2|0.001|0.279|0.189|Decrease tolerance to prevent premature stopping|\n",
    "|3|37|200|0.02|0.2|0.0001|0.269|0.204|Decrease dropout to improve training loss|\n",
    "|4|100|200|0.02|0.1|0.0001|0.218|0.2066|Increase learning rate to speed up training|\n",
    "|5|100|200|0.04|0.1|0.0001|0.210|0.171|Retain early stopping to prevent overfitting|\n",
    "|6|78|200|0.05|0.1|0.0001|0.209|0.183|-|\n",
    "These experiments revealed that modifying hyperparameters can lead to significant improvements in the model's performance. Specifically, we found that increasing the embedding size, decreasing the tolerance, and decreasing the dropout rate can all lead to improved performance. Moreover, increasing the learning rate can speed up the training process, while retaining early stopping can prevent overfitting. These insights can be valuable in selecting appropriate hyperparameters for future experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del W\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to support deeper architectures \n",
    "\n",
    "Extend the network to support back-propagation for more hidden layers. You need to modify the `backward_pass` function above to compute gradients and update the weights between intermediate hidden layers. Finally, train and evaluate a network with a deeper architecture. Do deeper architectures increase performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:58:51.764619Z",
     "start_time": "2020-04-02T14:58:47.483690Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (9855, 300)\n",
      "Shape W1 (300, 60)\n",
      "Shape W2 (60, 3)\n",
      "For epoch no. 1:\n",
      " Training loss = 1.0904073977205453\n",
      " Validation loss = 1.0835470429698713\n",
      "For epoch no. 2:\n",
      " Training loss = 1.0683178028837397\n",
      " Validation loss = 1.065800126175885\n",
      "For epoch no. 3:\n",
      " Training loss = 1.031747951240885\n",
      " Validation loss = 1.0242542141235624\n",
      "For epoch no. 4:\n",
      " Training loss = 0.9699242424008484\n",
      " Validation loss = 0.9536069160237284\n",
      "For epoch no. 5:\n",
      " Training loss = 0.8773791569292998\n",
      " Validation loss = 0.8660552498066273\n",
      "For epoch no. 6:\n",
      " Training loss = 0.7525390488002102\n",
      " Validation loss = 0.7371910204662908\n",
      "For epoch no. 7:\n",
      " Training loss = 0.6269514847308124\n",
      " Validation loss = 0.6581247178305972\n",
      "For epoch no. 8:\n",
      " Training loss = 0.505625677991914\n",
      " Validation loss = 0.6069301765466671\n",
      "For epoch no. 9:\n",
      " Training loss = 0.41873556322847433\n",
      " Validation loss = 0.507125280774279\n",
      "For epoch no. 10:\n",
      " Training loss = 0.34188897718964223\n",
      " Validation loss = 0.42300718931129244\n",
      "For epoch no. 11:\n",
      " Training loss = 0.2864840089189774\n",
      " Validation loss = 0.40379728292747036\n",
      "For epoch no. 12:\n",
      " Training loss = 0.24529165733031572\n",
      " Validation loss = 0.3763314325506789\n",
      "For epoch no. 13:\n",
      " Training loss = 0.21509846572932784\n",
      " Validation loss = 0.32584063411630865\n",
      "For epoch no. 14:\n",
      " Training loss = 0.18467657428925607\n",
      " Validation loss = 0.31187106754210575\n",
      "For epoch no. 15:\n",
      " Training loss = 0.16261490801847295\n",
      " Validation loss = 0.3006096674558227\n",
      "For epoch no. 16:\n",
      " Training loss = 0.14753155912330498\n",
      " Validation loss = 0.27724778773849346\n",
      "For epoch no. 17:\n",
      " Training loss = 0.1328422176053359\n",
      " Validation loss = 0.2903833989906615\n",
      "For epoch no. 18:\n",
      " Training loss = 0.11829466003477684\n",
      " Validation loss = 0.265171091017501\n",
      "For epoch no. 19:\n",
      " Training loss = 0.10861963508128739\n",
      " Validation loss = 0.2678327278978684\n",
      "For epoch no. 20:\n",
      " Training loss = 0.10575483949232259\n",
      " Validation loss = 0.23721789804504204\n",
      "For epoch no. 21:\n",
      " Training loss = 0.09581765140385896\n",
      " Validation loss = 0.22684371231100253\n",
      "For epoch no. 22:\n",
      " Training loss = 0.0787146353477721\n",
      " Validation loss = 0.2780076189067811\n",
      "For epoch no. 23:\n",
      " Training loss = 0.08700687217209986\n",
      " Validation loss = 0.22864465268161174\n",
      "For epoch no. 24:\n",
      " Training loss = 0.08049197825213504\n",
      " Validation loss = 0.21305223640537996\n",
      "For epoch no. 25:\n",
      " Training loss = 0.07385595322534019\n",
      " Validation loss = 0.2235662828639378\n",
      "For epoch no. 26:\n",
      " Training loss = 0.06963481060082467\n",
      " Validation loss = 0.21409893508349676\n",
      "For epoch no. 27:\n",
      " Training loss = 0.06593387929855672\n",
      " Validation loss = 0.18962027030211806\n",
      "For epoch no. 28:\n",
      " Training loss = 0.06270590021789964\n",
      " Validation loss = 0.24835844635367021\n",
      "For epoch no. 29:\n",
      " Training loss = 0.05838767808317667\n",
      " Validation loss = 0.20733402245314847\n",
      "For epoch no. 30:\n",
      " Training loss = 0.05727786559936547\n",
      " Validation loss = 0.1837914097073553\n",
      "For epoch no. 31:\n",
      " Training loss = 0.05648026943501338\n",
      " Validation loss = 0.2796591104631622\n",
      "For epoch no. 32:\n",
      " Training loss = 0.05200353926769028\n",
      " Validation loss = 0.2044358457773332\n",
      "For epoch no. 33:\n",
      " Training loss = 0.05294114907046588\n",
      " Validation loss = 0.22081144037332456\n",
      "For epoch no. 34:\n",
      " Training loss = 0.048022391864711717\n",
      " Validation loss = 0.2376851923604117\n",
      "For epoch no. 35:\n",
      " Training loss = 0.04756496497573593\n",
      " Validation loss = 0.21054459559485433\n",
      "For epoch no. 36:\n",
      " Training loss = 0.04243550187059789\n",
      " Validation loss = 0.22952282667620189\n",
      "For epoch no. 37:\n",
      " Training loss = 0.04164323938297865\n",
      " Validation loss = 0.21894913561741805\n",
      "For epoch no. 38:\n",
      " Training loss = 0.044674700896508386\n",
      " Validation loss = 0.2026341446365173\n",
      "For epoch no. 39:\n",
      " Training loss = 0.03908045177023369\n",
      " Validation loss = 0.1744407643951558\n",
      "For epoch no. 40:\n",
      " Training loss = 0.041221685414628444\n",
      " Validation loss = 0.2489769683437778\n",
      "For epoch no. 41:\n",
      " Training loss = 0.03763577809414296\n",
      " Validation loss = 0.20217877962411432\n",
      "For epoch no. 42:\n",
      " Training loss = 0.03605173735589715\n",
      " Validation loss = 0.22963178515210086\n",
      "For epoch no. 43:\n",
      " Training loss = 0.03605426169653284\n",
      " Validation loss = 0.19407772936157736\n",
      "For epoch no. 44:\n",
      " Training loss = 0.03622933762835704\n",
      " Validation loss = 0.21607807839664453\n",
      "For epoch no. 45:\n",
      " Training loss = 0.03357116564893767\n",
      " Validation loss = 0.17741627091047416\n",
      "For epoch no. 46:\n",
      " Training loss = 0.033470541516221364\n",
      " Validation loss = 0.18591540116566302\n",
      "For epoch no. 47:\n",
      " Training loss = 0.029739176450105178\n",
      " Validation loss = 0.19120506233185403\n",
      "For epoch no. 48:\n",
      " Training loss = 0.03251596251019622\n",
      " Validation loss = 0.19652638930750763\n",
      "For epoch no. 49:\n",
      " Training loss = 0.03234802815060365\n",
      " Validation loss = 0.18163811238342487\n",
      "For epoch no. 50:\n",
      " Training loss = 0.03061067401967376\n",
      " Validation loss = 0.21195048176933295\n",
      "For epoch no. 51:\n",
      " Training loss = 0.02868973596381216\n",
      " Validation loss = 0.20329661648009906\n",
      "For epoch no. 52:\n",
      " Training loss = 0.029338563631480224\n",
      " Validation loss = 0.21474884992818732\n",
      "For epoch no. 53:\n",
      " Training loss = 0.02881805112331403\n",
      " Validation loss = 0.20228371979686013\n",
      "For epoch no. 54:\n",
      " Training loss = 0.027663585914199772\n",
      " Validation loss = 0.18609414726744947\n",
      "For epoch no. 55:\n",
      " Training loss = 0.02875653145562869\n",
      " Validation loss = 0.1837448599543129\n",
      "For epoch no. 56:\n",
      " Training loss = 0.02784215376831873\n",
      " Validation loss = 0.1904690943245129\n",
      "For epoch no. 57:\n",
      " Training loss = 0.025460112957434403\n",
      " Validation loss = 0.18130690098768598\n",
      "For epoch no. 58:\n",
      " Training loss = 0.023962356698385492\n",
      " Validation loss = 0.1876241794663753\n",
      "For epoch no. 59:\n",
      " Training loss = 0.023076753791822526\n",
      " Validation loss = 0.2018548957619824\n",
      "For epoch no. 60:\n",
      " Training loss = 0.023991091298890706\n",
      " Validation loss = 0.18611932083569785\n",
      "For epoch no. 61:\n",
      " Training loss = 0.022363719018301604\n",
      " Validation loss = 0.18536014661275724\n",
      "[0.9861176105679531, 0.011551947367884957, 0.0023304420641619265]\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(vocab),embedding_dim=300,\n",
    "                    hidden_dim=[60], num_classes=3)\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "W[0] = w_glove\n",
    "\n",
    "W, loss_tr, dev_loss = SGD(x_train, y_train,\n",
    "                            W,\n",
    "                            X_dev=x_dev, \n",
    "                            Y_dev=y_dev,\n",
    "                            lr=0.001, \n",
    "                            dropout=0.2,\n",
    "                            freeze_emb=False,\n",
    "                            tolerance=0.001,\n",
    "                            epochs=100)\n",
    "out = forward_pass(x_test[0],W)\n",
    "print(out['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:11:51.994986Z",
     "start_time": "2020-04-02T15:11:51.992563Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8922222222222222\n",
      "Precision: 0.8927750562280119\n",
      "Recall: 0.8922222222222222\n",
      "F1-Score: 0.8922839254111402\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y_pred'])+1 for x in x_test]\n",
    "print('Accuracy:', accuracy_score(y_test,preds_te))\n",
    "print('Precision:', precision_score(y_test,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(y_test,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(y_test,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of hyperparameter tuning involves adjusting the hyperparameters and monitoring the validation results to determine the best configuration. The following table presents the results of several runs with varying hyperparameters, along with comments regarding the observed behaviors:\n",
    "|# Run|# Number of Epochs|Hidden layer sizes|Learning rate|Dropout|Tolerance|Training loss|Validation loss| Comments\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|1|78|[100]|0.01|0.1|0.001|0.004|0.231|Deeper network led to overfitting|\n",
    "|2|62|[60,30]|0.01|0.1|0.001|0.004|0.209|Decreasing number of units prevented overfitting|\n",
    "|3|5|[30]|0.01|0.1|0.001|0.051|0.233|Early stopping resulted in suboptimal performance|\n",
    "|4|21|[60]|0.01|0.1|0.001|0.002|0.241|Oscillations were observed and the learning rate was decreased|\n",
    "|5|17|[60]|0.001|0.1|0.001|0.022|0.185|Best performance was achieved with this configuration|\n",
    "The table summarizes the results obtained from different hyperparameter configurations. In particular, it shows the number of epochs, the sizes of the hidden layers, the learning rate, the dropout rate, the tolerance, the training and validation losses, and any comments or observations regarding the model's performance. By analyzing these results, one can identify the optimal hyperparameters for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results\n",
    "\n",
    "Add your final results here:\n",
    "\n",
    "| Model | Precision  | Recall  | F1-Score  | Accuracy\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| Average Embedding  | 0.864  |  0.861 | 0.861 | 0.861  |\n",
    "| Average Embedding (Pre-trained)  | 0.882  | 0.881  |  0.881 |  0.881 |\n",
    "| Average Embedding (Pre-trained) + X hidden layers    |  0.893 | 0.892  | 0.892  | 0.892  |\n",
    "\n",
    "Please discuss why your best performing model is better than the rest and provide a bried error analaysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-The best performing model in this study is achieved by incorporating a pre-trained embedding matrix obtained from a larger dataset and a deeper neural network architecture. The initial model was observed to overfit, which suggests that the training dataset may be insufficient or a deeper network is required. To address this issue, a pre-trained embedding matrix was utilized without further updates, resulting in improved performance. The use of a pre-trained embedding matrix is a common strategy in natural language processing tasks as it allows the model to leverage knowledge learned from a larger dataset.\n",
    "Furthermore, a deeper neural network architecture was implemented in conjunction with the pre-trained embedding matrix to uncover additional latent features and further improve performance on test data. It was observed that an excessively deep network could result in overfitting, which highlights the importance of carefully selecting the network architecture.\n",
    "The error analysis showed that a reduction in the learning rate in conjunction with an increase in the number of units resulted in better performance on the test data. This finding suggests that a smaller learning rate may be more appropriate for this task to prevent the model from overfitting.\n",
    "Overall, the incorporation of a pre-trained embedding matrix and a deeper neural network architecture produced superior results compared to other models. These findings demonstrate the importance of selecting appropriate architectures and hyperparameters in machine learning models to optimize performance. However, it should be noted that the performance of the model is heavily dependent on the quality and quantity of the data used for training, and more data may be required to improve the model's performance further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "1. Overview of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 2400 \n",
      "Development examples 150 \n",
      "Testing examples 900\n",
      "Percentages: (69.56521739130434, 4.3478260869565215, 26.08695652173913)\n",
      "Max no. of features in training 95 \n",
      "Development 42 \n",
      "Testing 54\n",
      "Classes in training set:\n",
      "class 1: 800 examples\n",
      "class 2: 800 examples\n",
      "class 3: 800 examples\n",
      "Classes in development set:\n",
      "class 1: 50 examples\n",
      "class 2: 50 examples\n",
      "class 3: 50 examples\n",
      "Classes in training set:\n",
      "class 1: 300 examples\n",
      "class 2: 300 examples\n",
      "class 3: 300 examples\n"
     ]
    }
   ],
   "source": [
    "m_train, m_dev, m_test = len(y_train),len(y_dev),len(y_test)\n",
    "total = m_train + m_dev + m_test\n",
    "print(\"Training examples: %s \\nDevelopment examples %s \\nTesting examples %s\" %(m_train, m_dev, m_test))\n",
    "print(\"Percentages: (%s, %s, %s)\" %(m_train/total*100,m_dev/total*100,m_test/total*100))\n",
    "print(\"Max no. of features in training %s \\nDevelopment %s \\nTesting %s\" %(max([len(x) for x in x_train]),\n",
    "                                                max([len(x) for x in x_dev]),max([len(x) for x in x_test])))\n",
    "print(\"Classes in training set:\\nclass 1: %s examples\\nclass 2: %s examples\\nclass 3: %s examples\"\n",
    "       %(len([y for y in y_train if y==1]),len([y for y in y_train if y==2]),len([y for y in y_train if y==3])))\n",
    "print(\"Classes in development set:\\nclass 1: %s examples\\nclass 2: %s examples\\nclass 3: %s examples\"\n",
    "       %(len([y for y in y_dev if y==1]),len([y for y in y_dev if y==2]),len([y for y in y_dev if y==3])))\n",
    "print(\"Classes in training set:\\nclass 1: %s examples\\nclass 2: %s examples\\nclass 3: %s examples\"\n",
    "       %(len([y for y in y_test if y==1]),len([y for y in y_test if y==2]),len([y for y in y_test if y==3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we understand from the outputs, we understand that the dataset is split to 70% training, 4% development and 26% testing sets, the maximum no. of features are in training set, and the labels are equally distriputed between the 3 classes in each of the datasets (stratified distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Analyzation of the test performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model reached relativley very good testing scores (presision, recall, accuracy, and F1-score, all above 0.89 which is great), however we can always think of a way to make it even better, we can examine the testing correlation between true and predicted values using scipy.stat tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coeffiection: 0.7698464336630295\n",
      "Spearman correlation: SpearmanrResult(correlation=0.7693654642860066, pvalue=5.778606910234984e-177)\n"
     ]
    }
   ],
   "source": [
    "print(\"Pearson correlation coeffiection: %s\" %pearsonr(y_test,preds_te)[0])\n",
    "print(\"Spearman correlation: \"+str(spearmanr(y_test,preds_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that the results are strongly correlated, even though there are wrong predections, this means that maybe some of the labels are strongly correlated to each other rather than other labels.\n",
    "We can also evaluate the confusion matrix to better understand the testing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, preds_te)\n",
    "ncm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "plt.imshow(ncm,cmap=\"Blues\")\n",
    "plt.title(\"Normalized confusion matrix\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "for i in range(len(ncm)):\n",
    "    for j in range(len(ncm[0])):\n",
    "        plt.text(j,i,np.round(ncm[i,j],2),ha=\"center\",va=\"center\",color=\"k\")\n",
    "plt.xticks([0,1,2],labels=[\"1\",\"2\",\"3\"])\n",
    "plt.yticks([0,1,2],labels=[\"1\",\"2\",\"3\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above confussion matrix that the confusion is mostly occurring between the label 1 (which is politics) and the label 3 (which is economy), which actually makes sense, because some keywords may be common between these two topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Looking deeply into the mislabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled examples from topic 1: \n",
      "rivals possible fraud government says Chavez defeat produce turmoil world oil market\n",
      "AFP India Company Ltd took step expand Asian announcement buy Asia Pacific steel operations Singapore Ltd\n",
      "The price oil continued sharp rise overnight closing record high The main contract New York light sweet crude delivery next month closed record barrel up cents yesterday close\n",
      "SAN JOSE Calif In sign Google Inc initial public offering isn popular expected company lowered estimated price range between per share down earlier prediction per share\n",
      "NEW YORK Investors shrugged off rising crude futures Wednesday capture well priced shares sending Nasdaq index up percent ahead Google Inc much anticipated initial public offering stock In afternoon trading Dow Jones industrial average gained percent\n",
      "\n",
      "Mislabeled examples from topic 3: \n",
      "urban images giant sense how Houston many residents\n",
      "Vermont Republican governor challenged Bush administration prescription drug policy federal court yesterday first time state chosen legal battle over Canadian imports\n",
      "WASHINGTON Reuters The Food plans update labels suggest link between drugs suicide cautious about strength such ties according documents released Friday\n",
      "Venezuela The results audit support official vote count showing President Hugo Chavez won month recall referendum Venezuela head Organization American States said Saturday\n",
      "Venezuela President Hugo Chavez announced longer Democratic CD opposition coalition\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(x_test, dtype=object)\n",
    "Y_test = np.array(y_test)\n",
    "Y_pred = np.array(preds_te)\n",
    "y_1 = np.where(Y_test == 1)[0].tolist()\n",
    "y_3 = np.where(Y_test == 3)[0].tolist()\n",
    "ypred_1 = np.where(Y_pred == 1)[0].tolist()\n",
    "ypred_3 = np.where(Y_pred == 3)[0].tolist()\n",
    "mislabeled_1 = [value for value in y_1 if value in ypred_3]\n",
    "mislabeled_3 = [value for value in y_3 if value in ypred_1]\n",
    "X_mislabeled_1_words = [[id_word[id] for id in x] for x in X_test[mislabeled_1]]\n",
    "mislabeled_1 = [' '.join(words) for words in X_mislabeled_1_words]\n",
    "print(\"Mislabeled examples from topic 1: \\n%s\" %'\\n'.join(mislabeled_1[:5]))\n",
    "X_mislabeled_3_words = [[id_word[id] for id in x] for x in X_test[mislabeled_3]]\n",
    "mislabeled_3 = [' '.join(words) for words in X_mislabeled_3_words]\n",
    "print(\"\\nMislabeled examples from topic 3: \\n%s\" %'\\n'.join(mislabeled_3[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can see some words being reused in both topics (politics and economy), like government, policy, federal, democratic, ..etc. These words can be used in both topics and it makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Suggestions to improve the network performance:\n",
    "- We understood that the misclassification is generally happening between topics 1 and 3\n",
    "- We have known that some words maybe used repeatedly in both topics, and that explains how confusion matrix was\n",
    "- The first obvious suggestion is to increase the training datasets in replace of the test datasets 75%, 20% (and dev remains 4%)\n",
    "- We knew that the labels (1, 2, 3) are equally distriputed through all datasets, so another good suggestion would be to increase the precentage of labels 1, 3 in the training datasets in exchange with other labels (label 2), and not to let it be equally distributed\n",
    "- Another last suggestion is to try deeper architecures to help recognize more features (more words), however it's more preferable to use bigrams in addition to unigrams instead of this step, this will be a more gauranteed effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
